{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random variables: distributions and sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we recall some classical random variables (discrete and continuous) which are typically used in several real application in Bayesian modeling. We give their distributions and derive theoretically some basic characteristics. We will then sample from these distributions and verify empirically the previously obtained quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete distributions: \n",
    "\n",
    "Discrete random variables take values over a countable set of possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli distribution $(\\theta)$:\n",
    "\n",
    "The Bernoully random variable is typically used to describe an experiment in which you could have 2 outputs (success and failure). It depends on one parameter, $\\theta$. Let $X\\sim\\mathcal{B}(\\theta)$, then $X$ takes the value 1 with probability $\\theta$ and the value 0 with probability $1-\\theta$. \n",
    "\n",
    "$p(X=1)=\\theta=1-p(X=0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# We will use the scipy.stats module for rvs simulation: in this case we want to simulate a bernoulli rdv\n",
    "\n",
    "\n",
    "# Provide a value for the parameter of the bernoulli distribution\n",
    "theta = \n",
    "# And the number of samples\n",
    "N = \n",
    "\n",
    "# Generate N Bernoulli trials using the rvs method from scipy.stats. We will save results in the vector x below.\n",
    "x = \n",
    "\n",
    "# Print frequence of occurrence of 1 in x to get the frequence of 1s in x:\n",
    "occurrences1 = np.count_nonzero(x == 1)\n",
    "\n",
    "print(f'Frequnce of 1s = {occurrences1/N}')\n",
    "\n",
    "# Evaluate theoretical mean and variance, using the mean and var methods \n",
    "# (or alteratively with the theoretical frmulation)\n",
    "mean_x = \n",
    "var_x = \n",
    "\n",
    "print(f'Mean = {mean_x}')\n",
    "print(f'Variance = {var_x}')\n",
    "\n",
    "# Sampling mean and variance\n",
    "sample_mean = np.mean(x)\n",
    "sample_var = np.var(x)\n",
    "print('The sampling mean and variance are respectively {0:.2f} and {1:.2f}'.format(sample_mean, sample_var))\n",
    "\n",
    "#Plot: we will use histplot from seaborn module\n",
    "ax= sns.histplot(x,\n",
    "                 kde=False,\n",
    "                 color=\"skyblue\",\n",
    "                 binwidth=.1,\n",
    "                 alpha = 1)\n",
    "ax.set(xlabel='Bernoulli', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Derive the expectation and variance for a random variable $X\\sim\\mathcal{B}(\\theta)$. Than check that it corresponds with the one obtained previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial distribution $(n,\\theta)$:\n",
    "\n",
    "The Binomial distribution is the random variable resulting from the sum of $n$ independent Bernoulli random variables: it counts the number of success after $n$ independent Bernoulli trials with parameter $\\theta$. Let $X\\sim\\textrm{Binomial}(n,\\theta)$, then $X:=\\sum_{i=1}^nY_i$, where $Y_i$ are iid $\\mathcal{B}(\\theta)$. $X$ can take any value in the set $\\{0,\\dots,n\\}$.\n",
    "\n",
    "For all $k=0,\\dots,n$: $p(X=k)={{n}\\choose{k}}\\theta^k(1-\\theta)^{n-k}$, where ${{n}\\choose{k}}:=\\frac{n!}{k!(n-k)!}$ is the binomial coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Generate $N$ Binomial experiments with given $n, \\theta$ and compare the theoretical and the sampling mean and variance. Comment the frequency plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Import the req0uired method from scipy.stats to generate vernoulli rvs\n",
    "\n",
    "\n",
    "# Provide parameters and number of samples\n",
    "theta = \n",
    "n = \n",
    "N = \n",
    "\n",
    "# Generate N Binomial experiments \n",
    "x = \n",
    "\n",
    "# Evaluate mean and variance\n",
    "mean_x = \n",
    "var_x = \n",
    "\n",
    "print(f'Mean = {mean_x}')\n",
    "print(f'Variance = {var_x}')\n",
    "\n",
    "# Sampling mean and variance\n",
    "sample_mean = \n",
    "sample_var = \n",
    "print('The sampling mean and variance are respectively {0:.2f} and {1:.2f}'.format(sample_mean, sample_var))\n",
    "\n",
    "# Plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Derive the expectation and variance for a random variable $X\\sim\\textrm{Binomial}(n,\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous distributions: \n",
    "\n",
    "We introduce some continuous random variables supported on $\\mathbb{R}$ or on intervals of the real line. Continuous random variables are typically characterized through their *probability density function* (pdf), which describes somehow the probability that the random variable falls within a specific interval. Indeed, in the continuous case, the probability that $X$ is equal to a specific point in the real line will be always equal to zero due to the dimension of the considered interval. \n",
    "\n",
    "Let us supposed that $X$ is a continuous random variable defined over $\\mathbb{R}$, $\\textrm{pdf}_X$ its probability density function:\n",
    "\n",
    "$p(a\\leq X\\leq b)=\\int_a^b \\textrm{pdf}_X(x)dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform distribution $(a,b)$:\n",
    "\n",
    "Given an interval $[a,b]\\in\\mathbb{R}$, a uniform random variable over $[a,b]$ can takes with equal (uniform) probability any value in the interval $[a,b]$.  \n",
    "\n",
    "Let $X\\sim\\mathcal{U}(a,b)$, then $\\textrm{pdf}_X(x) = \\frac{1}{b-a}\\mathbb{I}_{[a,b]}(x)$,\n",
    "\n",
    "where $\\mathbb{I}_{[a,b]}(x)$ denotes the indicator function, $\\mathbb{I}_{[a,b]}(x)=1$ if $x\\in[a,b]$, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "###########\n",
    "\n",
    "\n",
    "x_lin = np.linspace(uniform.ppf(0.01,loc=a, scale=b-a),uniform.ppf(0.99,loc=a, scale=b-a), 100)\n",
    "\n",
    "ax= sns.histplot(x,\n",
    "                 kde=False,\n",
    "                 color=\"skyblue\",\n",
    "                 alpha=1,\n",
    "                 stat='density'\n",
    "                )\n",
    "\n",
    "# set limits for better visualization\n",
    "ax.set_xlim([a-1, b+1])\n",
    "\n",
    "# Plot of the probability density function\n",
    "ax.plot(x_lin, uniform.pdf(x_lin,loc=a, scale=b-a),'r-', lw=5, alpha=0.6, label='uniform pdf')\n",
    "ax.set(xlabel='Uniform', ylabel='Frequency')\n",
    "ax.legend(loc='best', frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Derive the expectation and variance for a random variable $X\\sim\\mathcal{U}(a,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta distribution $(\\alpha,\\beta)$:\n",
    "\n",
    "The Beta distribution is defined on the interval $[0,1]$. It is the *conjugate prior probability distribution* for the Bernoulli and Binomial distributions (we will talk later about conjugate prior probability distributions). \n",
    "\n",
    "Let $X\\sim\\textrm{Beta}(\\alpha,\\beta)$, then $\\textrm{pdf}_X(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\mathbb{I}_{[0,1]}(x)$, \n",
    "\n",
    "where $B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the beta function, \n",
    "\n",
    "and $\\Gamma(z)=\\int_0^\\infty y^{z-1} e^{-y} dy$ is the gamma function. \n",
    "\n",
    "Note that for all positive integer $n$, $\\Gamma(n)=(n-1)!$\n",
    "\n",
    "It holds true that: $\\Gamma(z+1)=z\\Gamma(z)$.\n",
    "\n",
    "We can prove that: $B(\\alpha,\\beta)=\\int_0^1 x^{\\alpha-1}(1-x)^{\\beta-1}\\,dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Generate $N$ random numbers from a Beta distribution with given parameters $\\alpha,\\beta$. Verify if the sample and theoretical mean and variance are in line and compare the normalized frequency histogram with the Beta pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Derive the expectation and variance for a random variable $X\\sim\\textrm{Beta}(\\alpha,\\beta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** What happen when $\\alpha=\\beta=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal (or Gaussian) distribution $(\\mu,\\sigma^2)$:\n",
    "\n",
    "The normal or Gaussian distribution is supported on the entire real line. It appears to be vary useful in several real applications. It is an indispensable distribution in statistics, due also to the importance of results such as the *central limit theorem*. \n",
    "\n",
    "Let $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$, then for all $x\\in\\mathbb{R}$, $\\textrm{pdf}_X(x)=\\frac{1}{\\sigma\\sqrt(2\\pi)}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Without doing any calculation, what can we expect for the mean of $X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Derive the expectation and variance for a random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$. \n",
    "\n",
    "*Tip.* Use the moment generating function.\n",
    "\n",
    "$M_X(t)=\\mathbb{E}(e^{tX})=\\int_{\\mathbb{R}}e^{tx}\\textrm{pdf}_X(x)dx$\n",
    "\n",
    "It holds true that for all $k$:\n",
    "\n",
    "$\\mathbb{E}(X^k)=\\frac{d^k}{dt^k} M_X(t)\\Big|_{t=0}$\n",
    "\n",
    "\n",
    "*Recall.* The gaussian integral: $\\int_{\\mathbb{R}}exp\\left(-v^2\\right)dv=\\sqrt{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "M_X(t) & = & \\mathbb{E}(e^{tX})=\\int_{\\mathbb{R}}e^{tx}\\textrm{pdf}_X(x)dx \\\\\n",
    "& = & \\frac{1}{\\sigma\\sqrt(2\\pi)}\\int_{\\mathbb{R}}exp\\left(tx{-\\left(\\frac{x-\\mu}{\\sqrt{2}\\sigma}\\right)^2}\\right)dx\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: The central limit theorem\n",
    "\n",
    "Let $(X_n)_{n\\geq 1}$ be iid random variables with mean $E(X)$ and variance $Var(X)$. Then $\\sqrt{n}\\left(\\frac{\\overline{X}_n-E(X)}{\\sqrt{Var(X)}}\\right)$ converges through a standard normal random variable:\n",
    "\n",
    "$$p\\left(a<\\sqrt{n}\\left(\\frac{\\overline{X}_n-E(X)}{\\sqrt{Var(X)}}\\right)<b\\right)\\rightarrow\\int_a^b\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of Gaussian distributed data can be found everywhere, and in some sense the Gaussian distribution is a natural approximation for many phenomena observable in the realm of data analysis. This is especially the case for measurements that *cumulate*, for example over multiple subjects or probes. This is formally stated by the central limit theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.stats import norm\n",
    "from  scipy.stats import bernoulli\n",
    "\n",
    "path_length = 20\n",
    "N_subjects = 50\n",
    "\n",
    "prob_move = 0.5\n",
    "\n",
    "sample = bernoulli.rvs(prob_move, size = path_length * N_subjects)\n",
    "\n",
    "paths = np.cumsum(sample.reshape(N_subjects,path_length),1)\n",
    "\n",
    "[plt.plot(np.arange(path_length), paths[i,:], color = 'blue', lw = 0.1) for i in range(N_subjects)]\n",
    "plt.title('Trajectories')\n",
    "plt.xlabel('step')\n",
    "plt.xlabel('position')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Applying central limit theorem\n",
    "bernoulli_mean = prob_move\n",
    "bernoulli_var = prob_move * (1-prob_move)\n",
    "\n",
    "normal_clt_mean = path_length * bernoulli_mean\n",
    "normal_clt_var = path_length * bernoulli_var\n",
    "\n",
    "y = paths[:,path_length-1]\n",
    "\n",
    "plt.hist(y, bins = 14, density = True, label = 'Target distribution')\n",
    "\n",
    "x = np.arange(0,20,0.1)\n",
    "plt.plot(x, norm.pdf(x, normal_clt_mean, np.sqrt(normal_clt_var)), label = 'Gaussian clt approximation')\n",
    "plt.title('Distribution of target positions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link between the Bernoulli/Binomial distribution and the Beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider $n$ indipendent Bernoulli trials and the observation $y$ being the number of successes (*e.g.* the observation $y$ corresponds to the number of heads out of $n$ toss coin). We want to estimate the parameter $\\theta$, which represents the probability of success of each coin toss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea at the basis of the bayesian approach is that the parameters to be estimated are not fixed (*i.e.* it actually exists a **true** value to be found), but they are instead **random variables** and we dispose of some information on its *prior distribution* (which can be more or less informative depending on our degree of confidence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the Bayesian rule, we have\n",
    "\n",
    "$$p(\\theta|y)=\\frac{p(y|\\theta)p(\\theta)}{p(y)}\\propto p(y|\\theta)p(\\theta)$$\n",
    "\n",
    "since $p(y)$ is independent from $\\theta$, hence we could forget about that at least for the moment (it is simply a normalizing factor ensuring that the probability distribution gives 1 when integrated over all possible $\\theta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y|\\theta)$ is easy to determine, since $y$ follows a Binomial distribution, $\\textrm{Binomial}(n,\\theta)$, hence:\n",
    "\n",
    "$$p(y|\\theta)={{y} \\choose {n}}\\theta^y(1-\\theta)^{n-y}\\propto \\theta^y(1-\\theta)^{n-y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that the Beta distribution provide us with an elegant way to specify a prior in Binomial models. \n",
    "\n",
    "By definition, $\\theta$ should be between 0 and 1, so a good prior for $\\theta$ should be a distribution whose support is $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Which distribution can you propose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Determine the posterior distribution given $p(\\theta)=\\textrm{Beta}(\\alpha,\\beta)$ (up to a costant factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Can you see what distribution is this? Determine the right normalizing factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
