---
layout: archive
permalink: /teaching/model-selection/
title: "Model Selection"
author_profile: true
---

Requirements:
- Minimal background in mathematics and statistic
- Analysis and calculus (integral, derivatives, study of functions, … )
- Basic statistics concepts 
- Basic expertise with Python and Jupyter Notebook


Course Material all the material is available at this page.

|  Lesson           | Support Slides    | Notebook | Additional Material | References | 
| ----------------- | ----------------- | -------- | ----------- | ---------- | 
|Day 1 | [Introduction](https://marcolorenzi.github.io/material/Resampling/intro.pdf)           |  [Basic Probability Models and Sampling in Python](https://marcolorenzi.github.io/material/Resampling/Lesson1.ipynb)     |      /       |         /   |
|Day 2 | /          |  [Data Generation - Regression & Classification](https://marcolorenzi.github.io/material/Resampling.ipynb)     |      /       |     [Gu2003], [Gu2007]   |


References:

   <h4> References </h4>
      
      <p> [Gu2003] <i> Design of experiments for the NIPS 2003 variable selection benchmark</i>.  I. Guyon, 2003. <a href="http://clopinet.com/isabelle/Projects/NIPS2003/Slides/NIPS2003-Datasets.pdf"> link </a> </p>
      <p> [Gu2007] <i> Competitive baseline methods set new standards for the NIPS 2003 feature selection benchmark</i>. I. Guyon, J. Li, T. Mader, P.A. Pletscher, G. Schneider, M. Uhr. Pattern Recognition Letters, 28, 1438-1444, 2007. </p>
      <p> [HTF2001] <i> The Elements of Statistical Learning</i>. T. Hastie, R. Tibshirani, and J. Friedman. Springer Series in Statistics Springer New York Inc., New York, NY, USA, 2001.</p>
      <p> [Bis2006] <i> Pattern Recognition and Machine Learning</i>. 	C.M. Bishop. Springer-Verlag Berlin, Heidelberg, DE, 2006.	
      </p>
      <p> [Gem1992] <i> Neural networks and the bias/variance dilemma</i>. 	S. Geman, E. Bienenstock, R Doursat. Neural Computation, 4:2, 1-58, 1992. </p>	
      <p> [Brei1996] <i> Bagging predictors </i>. L. Breiman. Machine learning, 24(2), 123-140, 1996 </p>
      <p> [Efr1986] <i>  Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy</i>. B. Efron,  R. Tibshirani, Statistical science, 54-75. 1986. </p>
      <p> [Koh1995] <i> A study of cross-validation and bootstrap for accuracy estimation and model selection</i>. R. Kohavi. IJCAI, 14:2, 1995.</p>
      <p> [Rao2008] <i> On the dangers of cross-validation. An experimental evaluation. </i> R. Barat Rao, G. Fung, and R. Rosales. Proceedings of the 2008 SIAM International Conference on Data Mining. Society for Industrial and Applied Mathematics, 2008.  </p>
      <p> [McE2016] <i> Statistical Rethinking. A Bayesian Course with Examples in R and Stan. </i> R. McElreath. T&F Crc Press, 2016.  </p>
      <p> [Wag2004] <i> AIC model selection using Akaike weights.</i> E.J. Wagenmakers , S. Farrell. Psychon Bull Rev. 11(1):192-6, 2004.  </p>
      <p> [Sym2011]  <i> A brief guide to model selection, multimodel inference and model averaging in behavioural ecology using Akaike’s information criterion.</i> M.R. Symonds, A. Moussalli, A. Behavioral Ecology and Sociobiology, 65(1), 13-21, 2011. </p>

